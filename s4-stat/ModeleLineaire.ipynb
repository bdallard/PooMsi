{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle linéaire\n",
    "\n",
    "Considérons la cas classique d'une fonction affine :\n",
    "\n",
    "$$y=ax+b$$\n",
    "\n",
    "Ici, $a$ et $b$ sont des réels. Ces deux nombres définissent entièrement la courbe et permet donc d'obtenir une relation **affine** entre $x$ et $y$. En statistique, cette relation est à la base des modèles dit **linéaires**, où une variable réponse se définit comme une somme de variables explicatives où chacune de ces dernières sont multipliés par un coefficient.\n",
    "\n",
    "\n",
    "## Modèle linéaire simple\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png)\n",
    "\n",
    "Dans le modèle linéaire simple (une seule variable explicative), on suppose que la variable réponse suit le modèle suivant :\n",
    "\n",
    "$$y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n",
    "\n",
    "On remarque la ressemblance avec la fonction affine présentée ci-dessus. La différence réside dans l'existence du terme aléatoire (appelé bruit) $\\varepsilon_i$. Afin de considérer le modèle, il est nécessaire de se placer sous les hypothèses suivantes\n",
    "\n",
    "$$(\\mathcal{H}) : \\left\\{\\begin{matrix}\n",
    "\\mathbb{E}[\\varepsilon_i]=0\\\\ \n",
    "\\text{Cov}(\\varepsilon_i, \\varepsilon_j)=\\delta_{ij} \\sigma^2\n",
    "\\end{matrix}\\right.$$\n",
    "Les différents éléments qui interviennent sont :\n",
    "\n",
    "- $\\beta_0$ : l'ordonnée à l'origine (nommée *intercept*)\n",
    "- $\\beta_1$ : le coefficient directeur\n",
    "- $x_i$ : l'observation $i$\n",
    "- $y_i$ : le $i$-ème prix\n",
    "- $\\varepsilon_i$ : le bruit aléatoire liée à la $i$-ème observation\n",
    "\n",
    "La solution peut se calculer facilement via les formules fermées suivantes :\n",
    "\n",
    "$$\\hat{\\beta}_1=\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\qquad \\hat{\\beta}_0 = \\hat{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "## Modèle linéaire multiple\n",
    "\n",
    "Dans le cas multiple (pour $p$ variables explicatives), pour la $i$-ème observation, le modèle s'écrit :\n",
    "\n",
    "$$y_i= \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} + \\varepsilon_i$$\n",
    "\n",
    "Ainsi, une observation $x_i$ n'est plus une valeur, mais un **vecteur** $(x_{i1}, \\dots, x_{ip})$. Il est plus commode de regrouper ces prix $y_i$ et ces vecteurs d'observations $x_i$ dans des matrices :\n",
    "\n",
    "$$Y=X \\beta + \\varepsilon$$\n",
    "\n",
    "Sous les hypothèses équivalentes du modèle simple en plus grand dimension\n",
    "\n",
    "$$(\\mathcal{H}) : \\left\\{\\begin{matrix}\n",
    "\\text{rank}(X)=p\\\\ \n",
    "\\mathbb{E}[\\varepsilon]=0 \\text{ et }\\text{Var}(\\varepsilon)=\\sigma^2 I_p\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "Les différents éléments qui interviennent sont :\n",
    "\n",
    "- $\\beta$ : le vecteur directeur\n",
    "- $X$ : la matrice des observations\n",
    "- $Y$ : le vecteur de prix\n",
    "- $\\varepsilon$ : le vecteur de bruit\n",
    "\n",
    "Avec $X=( \\mathbf{1}, X_1, \\dots, X_n)$, $Y=(y_1, \\dots, y_n)^\\top$ et $\\varepsilon=(\\varepsilon_1, \\dots, \\varepsilon_n)^\\top$. La solution des MCO (Moindres Carrés Ordinaires) est alors :\n",
    "\n",
    "$$\\hat{\\beta}= (X^\\top X)^{-1} X^\\top Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP : Implémenter une régression linéaire \n",
    "\n",
    "Pour plus de détail sur les régressions voir le dossier `Sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model # Le modèle linéaire\n",
    "from sklearn.metrics import mean_squared_error, r2_score # Métriques d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données effectué.\n"
     ]
    }
   ],
   "source": [
    "prices = pd.read_csv(\"../data/price_availability.csv\", sep=\";\")\n",
    "listings = pd.read_csv(\"../data/listings_final.csv\", sep=\";\")\n",
    "print(\"Chargement des données effectué.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données d'entrée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif ici est de charger les données pour créer les matrices $X$ et $Y$ du modèle linéaire. **Attention**, il n'est pas nécessaire de rajouter le vecteur colonne $\\mathbf{1}$ en première colonne, car *scikit-learn* le fait automatiquement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Définir nos variables de travail \n",
    "X = listings.loc[:, [\"listing_id\", ... ]]\n",
    "Y = []\n",
    "\n",
    "for i, row in X.iterrows():\n",
    "    y = 0\n",
    "    # TODO\n",
    "    # Construire l'ensemble de donnée prix (astuce : faites des moyennes)\n",
    "    # Ajoutez les à la liste Y \n",
    "    \n",
    "# Autre possibilité : utiliser la fonction X.apply() pour récupérer les y !\n",
    "# (Plus difficile, mais plus optimisé)\n",
    "# Convertir Y en objet numpy \n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *Machine Learning*, on a l'habitude de couper l'ensemble de données en deux sous-ensembles :\n",
    "\n",
    "- Un ensemble d'entraînement (*train set*), sur lequel le modèle va être calibré.\n",
    "- Un ensemble de test (*test set*), qui ne sera pas utilisé pendant le calibrage mais permettra de vérifier l'aptitude du modèle à généraliser sur de nouvelles observations inconnues.\n",
    "\n",
    "En général, on découpe l'ensemble de données (*split*) en prenant $\\alpha \\%$ de l'ensemble pour entraînement et $1-\\alpha \\%$ comme test. Dans l'exemple suivant, on considère que $\\alpha=20 \\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:, 1:]\n",
    "X_test = X.iloc[800:, 1:]\n",
    "Y_train = Y\n",
    "Y_test = Y[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour information, *scikit-learn* utilise le solveur OLS (Ordinary Least Squares) de *numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO \n",
    "# Créer l'objet de régression\n",
    "regr = \n",
    "# On entraîne le modèle sur notre ensemble d'entraînement --> FIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche le vecteur des coefficients pour interpréter rapidement le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Afficher et commenter les coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation du modèle\n",
    "\n",
    "### Coefficient de détermination $R^2$\n",
    "\n",
    "Par la suite, nous ferons l'hypothèse de gaussianité sur les bruits. Dans l'idée, nous aimerions obtenir une valeur numérique qui nous indique à quel point la régression linéaire a un sens sur nos données. Pour cela, introduisons les notations suivantes :\n",
    "\n",
    "- $SCT=\\|Y-\\hat{y} \\mathbf{1}\\|^2$ est la somme des carrés totaux\n",
    "- $SCE=\\|\\hat{Y}-\\hat{y} \\mathbf{1}\\|^2$ est la somme des carrés expliqués\n",
    "- $SCR=\\|\\hat{\\varepsilon}\\|^2$ est la somme des carrés résiduels\n",
    "\n",
    "L'idée est de décomposer la somme des carrés totaux comme la somme des carrés que le modèle explique, en plus de la somme des carrés qui sont liés aux résidus (et donc que le modèle ne peut pas expliquer). On voit donc ici l'intérêt de calculer un coefficient à partir du $SCE$. Puisque l'on a la relation suivante :\n",
    "\n",
    "$$SCT=SCE+SCR \\text{ alors } 1=\\frac{SCE}{SCT}+\\frac{SCR}{SCT}$$\n",
    "\n",
    "Plus les résidus sont petits (et donc la régression est \"bonne\"), plus $SCR$ devient petit et donc $SCE$ devient grand. Le schéma inverse s'opère de la même façon. Dans le meilleur des cas, on obtient $SCR=0$ et donc $SCE=SCT$ d'où le premier membre vaut $1$. Dans le cas contraite, $SCE=0$ et automatiquement, le premier membre est nul. C'est ainsi que l'on définit le coefficient de détermination $R^2$ comme \n",
    "$$R^2=\\frac{SCE}{SCT}=1-\\frac{SCR}{SCT}$$\n",
    "Ainsi, $R^2 \\in [0,1]$. Plus $R^2$ est proche de $1$, plus la régression linéaire a du sens. Au contraire, si $R^2$ est proche de $0$, le modèle linéaire possède un faible pouvoir explicatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nY_pred = regr.predict(X_train)\\nprint(\"Mean squared error: %.2f\"\\n      % mean_squared_error(Y, Y_pred))\\n# Coefficient de détermination R2\\nprint(\\'Variance score: %.2f\\' % r2_score(Y, Y_pred))\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO\n",
    "# Afficher l'erreur des moindres carrées (MSE) \n",
    "# Affichier le R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de l'homoscédasticité\n",
    "\n",
    "L'analyse de l'homoscédasticité est primordiale : c'est en particulier elle qui nous permet de vérifier, à partir des résidus, si les bruits vérifient bien l'hypothèse $(\\mathcal{H})$. On calcule donc les **résidus studentisés**.\n",
    "\n",
    "$$t_i^*=\\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}_{(i)} \\sqrt{1-h_{ii}}}$$\n",
    "Avec $h_{ii}=\\{X(X^\\top X)^{-1} X^\\top\\}_{ii}=H_{ii}$ la matrice de projection sur l'hyperplan des variables. Plus précisément, $H$ est la matrice qui projette $Y$ sur l'espace engendré par les variables, soit $\\hat{Y}=HY$. De même, on considère $\\hat{\\sigma}_{(i)}$ l'estimateur de la variance du bruit en supprimant l'observation $i$ (par une méthode de validation croisée Leave-One-Out que nous ne détaillerons pas ici).\n",
    "\n",
    "Dans ce cas, on peut montrer que les résidus studentisés suivent une loi de Student à $n-p-1$ degrés de liberté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regr.predict(X_train)\n",
    "n = X_train.shape[0]\n",
    "p = 4\n",
    "residuals = np.abs(Y_train - Y_pred)\n",
    "H = np.matmul(X_train, np.linalg.solve(np.dot(X_train.T, X_train), X_train.T))\n",
    "std_hat = np.dot(residuals, residuals) / (n - p)\n",
    "standart_residuals = np.asarray([residuals[i] / np.sqrt(std_hat * (1 - H[i, i])) for i in range(len(residuals))])\n",
    "student_residuals = np.asarray([ standart_residuals[i] * np.sqrt((n - p - 1) / (n - p - standart_residuals[i]**2)) for i in range(n) ])\n",
    "cook = np.asarray([ H[i, i] * student_residuals[i] / (X_train.shape[1] * (1 - H[i, i])) for i in range(n) ])\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(Y_pred, student_residuals, s=12, c=\"white\", edgecolors=\"blue\")\n",
    "plt.plot([min(Y_pred), max(Y_pred)], [ scipy.stats.t.ppf(q=0.975, df=n-p-1), scipy.stats.t.ppf(q=0.975, df=n-p-1)], color=\"green\", alpha=0.6, label=\"Quantile de Student\")\n",
    "plt.title(\"Analyse de l’homoscédasticité\")\n",
    "plt.xlabel(\"Prédictions $\\hat{y}_i$\")\n",
    "plt.ylabel(\"Résidus studentisés $|t_i^*|$\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
